{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded commonsense dataset \n",
    "\n",
    "commonsense_data=pd.read_excel('/Users/juniorcedrictonga/RA_MBZUAI/gpt_4o_CCKG_expanded_prime_final_prompt.xlsx',sheet_name='Indonesia')\n",
    "commonsense_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning \n",
    "clean_commonsense_data=commonsense_data[['event','knowledge','relation','llm_result','location','sub_topic']].drop_duplicates()\n",
    "clean_commonsense_data=clean_commonsense_data[clean_commonsense_data['event'] != clean_commonsense_data['knowledge']]\n",
    "clean_commonsense_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-graph extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFS approach to extract paths even if the subgraph is cyclic but too consuming\n",
    "#all_paths = []\n",
    "#paths_with_relations = []\n",
    "#for source in G_sub.nodes:\n",
    "    #    stack = [(source, [source])]  # Pile pour DFS : (noeud courant, chemin actuel)\n",
    "        \n",
    "     #   while stack:\n",
    "    #        current_node, path = stack.pop()\n",
    "            \n",
    "            # Ajouter un chemin s'il est valide\n",
    "     #       if len(path) >= 3:\n",
    "      #          relations = [\n",
    "      #              G_sub.get_edge_data(path[i], path[i + 1])['relation']\n",
    "      #              for i in range(len(path) - 1)\n",
    "      #          ]\n",
    "      #          paths_with_relations.append((path, relations))\n",
    "      #          all_paths.append(path)\n",
    "      #      \n",
    "            # Étendre le chemin en explorant les voisins\n",
    "      #      for neighbor in G_sub.neighbors(current_node):\n",
    "       #         if len(path) + 1 > len(G_sub):  # Prévenir les boucles infinies dans des graphes cycliques\n",
    "      #              continue\n",
    "        #        stack.append((neighbor, path + [neighbor]))\n",
    "\n",
    "    # Structurer les données des chemins extraits\n",
    "    #paths_data = [\n",
    "       # {\n",
    "    #        'path': \" -> \".join(path),\n",
    "      #      'relations': \" - \".join(relations),\n",
    "      #      'sub_topic': G_sub.nodes[path[0]].get('sub_topic', 'N/A'),  \n",
    "     #       'path_len': len(path)\n",
    "    #    }\n",
    "    #    for path, relations in paths_with_relations\n",
    "    #    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=clean_commonsense_data\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    G.add_edge(row['event'], row['knowledge'], relation=row['relation'])\n",
    "\n",
    "sub_topics = df['sub_topic'].unique()\n",
    "global_paths_df = pd.DataFrame(columns=['path', 'relations', 'sub_topic', 'path_len'])\n",
    "\n",
    "subtopic_stats = {}\n",
    "\n",
    "for sub_topic in sub_topics:\n",
    "    df_sub = df[df['sub_topic'] == sub_topic]\n",
    "    G_sub = nx.DiGraph()\n",
    "    for _, row in df_sub.iterrows():\n",
    "        G_sub.add_edge(row['event'], row['knowledge'], relation=row['relation'],\n",
    "                       location=row['location'], sub_topic=row['sub_topic'])\n",
    "    \n",
    "    # stat per subtopics\n",
    "    num_nodes = G_sub.number_of_nodes()\n",
    "    num_edges = G_sub.number_of_edges()\n",
    "    degree_distribution = dict(G_sub.degree())  \n",
    "    average_degree = sum(degree_distribution.values()) / num_nodes if num_nodes > 0 else 0\n",
    "    \n",
    "    central_nodes = sorted(degree_distribution.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    subtopic_stats[sub_topic] = {\n",
    "        'num_nodes': num_nodes,\n",
    "        'num_edges': num_edges,\n",
    "        'average_degree': average_degree,\n",
    "        'central_nodes': central_nodes\n",
    "    }\n",
    "   # path extraction with len >= 3\n",
    "    if nx.is_directed_acyclic_graph(G_sub):\n",
    "        all_simple_paths = []\n",
    "        paths_with_relations = []\n",
    "        longest_path = nx.dag_longest_path(G_sub)\n",
    "        print(\"\\nLongest path : {}\".format(longest_path))\n",
    "        for source in G_sub.nodes:\n",
    "            for target in G_sub.nodes:\n",
    "                if source != target:\n",
    "                    # Extraction of path\n",
    "                    paths = list(nx.all_simple_paths(G_sub, source=source, target=target, cutoff=None))\n",
    "                    filtered_paths = [path for path in paths if len(path) >= 3]\n",
    "                    all_simple_paths.extend(filtered_paths)\n",
    "\n",
    "                    # Extraction of relations\n",
    "                    for path in filtered_paths:\n",
    "                        relations = [\n",
    "                            G_sub.get_edge_data(path[i], path[i + 1])['relation']\n",
    "                            for i in range(len(path) - 1)\n",
    "                        ]\n",
    "                        paths_with_relations.append((path, relations))\n",
    "        paths_data = [\n",
    "            {   'path': \" -> \".join(path),\n",
    "                'relations': \" - \".join(relations),\n",
    "                'sub_topic': sub_topic,   \n",
    "                'path_len':len(path)\n",
    "               \n",
    "            }\n",
    "            for path, relations in paths_with_relations\n",
    "        ]\n",
    "        global_paths_df = pd.concat([global_paths_df, pd.DataFrame(paths_data)], ignore_index=True)\n",
    "\n",
    "            #print(\"\\n path with (len >= 3) with relation:\")\n",
    "            #for path, relations in paths_with_relations:\n",
    "            #    print(f\"Path: {' -> '.join(path)}\")\n",
    "            #   print(f\"Relations: {' -> '.join(relations)}\")\n",
    "    else:\n",
    "        print(\"\\n graph is cyclic\")\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G_sub, seed=42) \n",
    "    nx.draw(G_sub, pos, with_labels=True, node_size=50, edge_color=\"blue\", alpha=0.7, font_size=8)\n",
    "    edge_labels = nx.get_edge_attributes(G_sub, 'relation')\n",
    "    nx.draw_networkx_edge_labels(G_sub, pos, edge_labels=edge_labels, font_size=8, font_color=\"red\")\n",
    "\n",
    "    plt.title(f\"Graph per sub-topics : {sub_topic}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nStatistics by sub-topics :\")\n",
    "for sub_topic, stats in subtopic_stats.items():\n",
    "    print(f\"\\nSub-topic : {sub_topic}\")\n",
    "    print(f\"- Number of nodes : {stats['num_nodes']}\")\n",
    "    print(f\"- Number of edge: {stats['num_edges']}\")\n",
    "    print(f\"- mean degree : {stats['average_degree']:.2f}\")\n",
    "    print(f\"- Most central node (per degre) :\")\n",
    "    for node, degree in stats['central_nodes']:\n",
    "        print(f\"  - {node}: {degree} connexions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "global_paths_df.to_excel('/Users/juniorcedrictonga/RA_MBZUAI/Cultural_Commonsense_Knowledge_Graph/path_per_countries_subtopics_test.xlsx',sheet_name='Indonesia',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 250\n",
    "min_per_subtopic = max(1, total_samples // global_paths_df['sub_topic'].nunique())\n",
    "grouped = global_paths_df.groupby('sub_topic', group_keys=False)\n",
    "samples = grouped.apply(lambda x: x.sample(n=min(len(x), min_per_subtopic), random_state=42))\n",
    "remaining = total_samples - len(samples)\n",
    "if remaining > 0:\n",
    "    additional_samples = global_paths_df.drop(samples.index).sample(n=remaining, random_state=42)\n",
    "    samples = pd.concat([samples, additional_samples])\n",
    "\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "samples.to_excel('/Users/juniorcedrictonga/RA_MBZUAI/Cultural_Commonsense_Knowledge_Graph/250_path_per_countries_subtopics.xlsx',sheet_name='Indonesia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison with others CKG : MAMGO and CANDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "threshold=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/Users/juniorcedrictonga/RA_MBZUAI/mango_dataset_v1.jsonl'\n",
    "mango_data=pd.read_json(file_path,lines=True)\n",
    "mango_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matching_assertinons_with_mango(mango_data, commonsense_data,model, threshold, use_just_indonesian_data_in_mango=True):\n",
    "    if use_just_indonesian_data_in_mango:\n",
    "        # Extract Indonesian data in Mango\n",
    "        terms = [\"Indonesia\", \"Southeast Asia\", \"Asia-Pacific\",\"Global South\",\"Indonesian\"]\n",
    "        regex_pattern = '|'.join(terms)\n",
    "        mango_data['contains_relevant_terms'] = mango_data['culture'].str.contains(regex_pattern, case=False, na=False)\n",
    "        relevant_rows = mango_data[mango_data['contains_relevant_terms']]\n",
    "    else:\n",
    "        relevant_rows=mango_data\n",
    "\n",
    "\n",
    "    # find overlapping with our dataset and relevant rows of mango\n",
    "    relevant_rows_embedd=model.encode(relevant_rows['assertion'].tolist())\n",
    "    commonsense_data_embedd=model.encode(commonsense_data['llm_result'].tolist())\n",
    "    similarities=model.similarity(commonsense_data_embedd,relevant_rows_embedd)\n",
    "    results=[]\n",
    "    for i, idx in tqdm(enumerate(commonsense_data.index)): \n",
    "        for j in range (len(relevant_rows)): \n",
    "            similarity = similarities[i][j].item()\n",
    "            if similarity > threshold:\n",
    "                results.append({\n",
    "                    'mango_assertion': relevant_rows['assertion'].iloc[j], \n",
    "                    'llm_result': commonsense_data.at[idx,'llm_result'],  \n",
    "                    'relation': commonsense_data.at[idx, 'relation'],  \n",
    "                    'similarity': similarity,\n",
    "                    'mango_concept': relevant_rows['concept'].iloc[j],\n",
    "                    'mango_culture': relevant_rows['culture'].iloc[j],\n",
    "                    'sub_topic': commonsense_data.at[idx, 'sub_topic'],\n",
    "                    'event': commonsense_data.at[idx, 'event'],\n",
    "                    'knowledge': commonsense_data.at[idx,'knowledge'],\n",
    "                    'commonsense_data_index':idx\n",
    "                })\n",
    "    \n",
    "    matches_df = pd.DataFrame(results)\n",
    "    return relevant_rows, matches_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idonesian_rows_in_mango, mango_matches_data=extract_matching_assertinons_with_mango(mango_data,\n",
    "                                                                                    clean_commonsense_data,model,threshold,\n",
    "                                                                                    use_just_indonesian_data_in_mango=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idonesian_rows_in_mango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mango_matches_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sample of mango that overlapped with our data is \n",
    "mango_matches_data['mango_assertion'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Candle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='/Users/juniorcedrictonga/RA_MBZUAI/candle_dataset_v1.jsonl'\n",
    "candle_data=pd.read_json(file_path,lines=True)\n",
    "candle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find overlapping with our dataset and relevant rows of candle\n",
    "commonsense_data=clean_commonsense_data\n",
    "candle_rows_embedd=model.encode(candle_data['assertion'].tolist())\n",
    "commonsense_data_embedd=model.encode(commonsense_data['llm_result'].tolist())\n",
    "similarities=model.similarity(commonsense_data_embedd,candle_rows_embedd)\n",
    "results=[]\n",
    "for i, idx in tqdm(enumerate(commonsense_data.index)): \n",
    "    for j in range (len(candle_data)): \n",
    "        similarity = similarities[i][j].item()\n",
    "        if similarity > threshold:\n",
    "            results.append({\n",
    "                'candle_assertion': candle_data['assertion'].iloc[j], \n",
    "                'llm_result': commonsense_data.at[idx,'llm_result'],  \n",
    "                'relation': commonsense_data.at[idx, 'relation'],  \n",
    "                'similarity': similarity,\n",
    "                'candle_facet': candle_data['facet'].iloc[j],\n",
    "                'candle_subject': candle_data['subject'].iloc[j],\n",
    "                'candle_domain':candle_data['domain'].iloc[j],\n",
    "                'candle_concepts':candle_data['concepts'].iloc[j],\n",
    "                'sub_topic': commonsense_data.at[idx, 'sub_topic'],\n",
    "                'event': commonsense_data.at[idx, 'event'],\n",
    "                'knowledge': commonsense_data.at[idx,'knowledge'],\n",
    "                'commonsense_data_index':idx\n",
    "            })\n",
    "\n",
    "candle_matches_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sample of candle that overlappes with our data is \n",
    "candle_matches_df['candle_assertion'].unique().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
