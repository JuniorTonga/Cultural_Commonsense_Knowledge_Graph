{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bf1924",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add80770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os \n",
    "import re\n",
    "import torch\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cf8f3",
   "metadata": {},
   "source": [
    "## STEP 1: Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6d82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cross_duplicates(df):\n",
    "    df['sorted_tuple'] = df.apply(lambda row: tuple(sorted([row['event'], row['knowledge']])), axis=1)\n",
    "\n",
    "    duplicated_mask = df.duplicated(subset='sorted_tuple', keep='first')\n",
    "    df_filtered = df[~duplicated_mask].drop(columns=['sorted_tuple'])\n",
    "    df_removed = df[duplicated_mask].drop(columns=['sorted_tuple'])\n",
    "\n",
    "    return df_filtered, df_removed\n",
    "\n",
    "def clean_dataset(file_path, output_path):\n",
    "    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    cleaned_sheets = {}\n",
    "    removed_sheets = {}\n",
    "    \n",
    "    for country, df in sheets.items():\n",
    "        df_cleaned = df.drop_duplicates(subset=['event', 'knowledge'])\n",
    "        df_cleaned = df_cleaned[df_cleaned['event'] != df_cleaned['knowledge']]\n",
    "        df_cleaned = df_cleaned[~df_cleaned[\"event\"].isin([\"oNext\", \"xNext\"]) & \n",
    "                                ~df_cleaned[\"knowledge\"].isin([\"oNext\", \"xNext\"])]\n",
    "        \n",
    "        df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "        df_result, df_result_removed = remove_cross_duplicates(df_cleaned)\n",
    "        cleaned_sheets[country] = df_result\n",
    "        removed_sheets[country] = df_result_removed\n",
    "    \n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        for country, df in cleaned_sheets.items():\n",
    "            df.to_excel(writer, sheet_name=country, index=False)\n",
    "    \n",
    "    print(f\"Fichier nettoyé sauvegardé sous : {output_path}\")\n",
    "    return cleaned_sheets, removed_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd193a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = '/Users/junior.tonga/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/arab_multilingual_extension.xlsx'\n",
    "output_file = '/Users/junior.tonga/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/cleaned_arab_multilingual_extension.xlsx'\n",
    "clean_dataset(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe015f",
   "metadata": {},
   "source": [
    "## STEP 2: Paths construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_paths_with_relations(paths_with_relations):\n",
    "    paths_with_relations = sorted(paths_with_relations, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    unique_paths = []\n",
    "\n",
    "    for path, relations in paths_with_relations:\n",
    "        if not any(\n",
    "            all(node in other_path for node in path) and len(path) < len(other_path)\n",
    "            for other_path, _ in unique_paths\n",
    "        ):\n",
    "            unique_paths.append((path, relations))\n",
    "    \n",
    "    return unique_paths\n",
    "\n",
    "def export_to_excel(file_path, dataframe, sheet_name):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                dataframe.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'écriture dans le fichier existant : {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            with pd.ExcelWriter(file_path, engine='openpyxl',mode='w') as writer:\n",
    "                dataframe.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la création du fichier Excel : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaf30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data=pd.read_excel('/Cultural_Commonsense_Knowledge_Graph/results/cleaned_monolingual_generation.xlsx',sheet_name='China')\n",
    "initial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f60471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_statistics_build_paths(file_path, initial_data_path, output_stats_path, output_paths_path, output_format='json',filter=True):\n",
    "    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "    statistics = {}\n",
    "    \n",
    "    \n",
    "    for country, df in (sheets.items()):\n",
    "        print(country)\n",
    "        initial_df=pd.read_excel(initial_data_path,sheet_name=country)\n",
    "        G = nx.DiGraph()\n",
    "            \n",
    "        for _, row in df.iterrows():\n",
    "            G.add_edge(row['event'], row['knowledge'], relation=row['relation'])\n",
    "            \n",
    "        sub_topics = df['sub_topic'].unique()\n",
    "        #topic=df['sub_topic'].unique()\n",
    "        global_paths_df = pd.DataFrame(columns=['path_with_relations', 'relations', 'sub_topic', 'path_len']) \n",
    "        all_nodes = set(df['event']).union(set(df['knowledge'])) \n",
    "        country_stats = {\n",
    "                'num_nodes': len(all_nodes), \n",
    "                'num_edges': G.number_of_edges(), \n",
    "                'num_assertions': G.number_of_edges(),\n",
    "                'subtopics': {}\n",
    "            }\n",
    "            \n",
    "        paths_data = []\n",
    "            \n",
    "        for sub_topic in sub_topics:\n",
    "            print(sub_topic)\n",
    "            df_sub = df[df['sub_topic'] == sub_topic]\n",
    "            df2 = initial_df[initial_df['sub_topic'] == sub_topic]\n",
    "            G_sub = nx.DiGraph()\n",
    "            for _, row in df_sub.iterrows():\n",
    "                G_sub.add_edge(row['event'], row['knowledge'], relation=row['relation'])\n",
    "                \n",
    "            remaining_nodes = set(G_sub.nodes)\n",
    "            missing_after_cleaning = set(df2['event'].unique()) - remaining_nodes\n",
    "            if missing_after_cleaning:\n",
    "                print(f\"❌ Missing nodes after transformation: {missing_after_cleaning}\")\n",
    "            else:\n",
    "                print(\"✅ All source nodes are present.\")\n",
    "            num_nodes = G_sub.number_of_nodes()\n",
    "            num_edges = G_sub.number_of_edges()\n",
    "            degree_distribution = dict(G_sub.degree())\n",
    "            average_degree = sum(degree_distribution.values()) / num_nodes if num_nodes > 0 else 0\n",
    "            central_nodes = sorted(degree_distribution.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                \n",
    "            paths_with_relations = []\n",
    "            for source in df2['event'].unique():\n",
    "                for target in G_sub.nodes:\n",
    "                    print(target)\n",
    "                    if source != target:\n",
    "                        try:\n",
    "                            paths = list(nx.all_simple_paths(G_sub, source=source, target=target))\n",
    "                            for path in paths:\n",
    "                                #len(path)=number node in the path\n",
    "                                if len(path) >= 2:\n",
    "                                    relations = [G_sub.get_edge_data(path[i], path[i+1])['relation'] for i in range(len(path) - 1)]\n",
    "                                    paths_with_relations.append((path, relations))\n",
    "                        except nx.NodeNotFound:\n",
    "                            continue\n",
    "                \n",
    "            if filter==True:\n",
    "                paths_with_relations_filtered = filter_paths_with_relations(paths_with_relations)\n",
    "            else:\n",
    "                paths_with_relations_filtered = paths_with_relations\n",
    "            paths_data.extend([\n",
    "                    {\n",
    "                        'path_with_relations': '\\n'.join([f\"{path[i]}---{relations[i]}--->{path[i+1]}\" for i in range(len(path)-1)]),\n",
    "                        'sub_topic': sub_topic,\n",
    "                        'relations': \" - \".join(str(r) for r in relations ),\n",
    "                        'path_len': len(path) - 1 \n",
    "                    }\n",
    "                    for path, relations in paths_with_relations_filtered\n",
    "                ])\n",
    "                \n",
    "            country_stats['subtopics'][sub_topic] = {\n",
    "                    'num_nodes': num_nodes,\n",
    "                    'num_edges': num_edges,\n",
    "                    'average_degree': average_degree,\n",
    "                    'central_nodes': central_nodes,\n",
    "                    'num_paths': len(paths_with_relations_filtered)\n",
    "                }\n",
    "            \n",
    "        statistics[country] = country_stats\n",
    "            \n",
    "            \n",
    "        if paths_data:\n",
    "            paths_df = pd.DataFrame(paths_data)\n",
    "            paths_df = paths_df.dropna() \n",
    "            global_paths_df = pd.concat([global_paths_df, paths_df], ignore_index=True)\n",
    "            export_to_excel(file_path=output_paths_path,dataframe=global_paths_df,sheet_name=country)\n",
    "\n",
    "    #Save some statistics\n",
    "    if output_format == 'json':\n",
    "        with open(output_stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(statistics, f, indent=4, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file = '/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/cleaned_arab_multilingual_extension.xlsx'\n",
    "output_stats_file = '/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/cleaned_arab_multilingual_extension_statistics.json'\n",
    "output_paths_file = '/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/ckg_data/cleaned_arab_multilingual_extension_paths_final.xlsx'\n",
    "initial_data_path='/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/cleaned_arab_initial_multilingual_generation.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_statistics_build_paths(file_path=input_file,initial_data_path=initial_data_path, \n",
    "                   output_stats_path=output_stats_file, output_paths_path=output_paths_file, output_format='json',filter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3218c",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Reformatted the paths by removing node relationships and extracting only the first node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55009673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_path(path):\n",
    "    if not path or pd.isna(path):\n",
    "        return None\n",
    "    \n",
    "    steps = path.split(\" --> \")\n",
    "    formatted_steps = [f\"- {steps[i]} → {steps[i+1]}\" for i in range(len(steps) - 1)]\n",
    "    \n",
    "    return \"\\n\".join(formatted_steps)\n",
    "\n",
    "def extract_first_node_and_reformat(path):\n",
    "    \"\"\"\n",
    "    Extracts the first node and reformats the path by removing relations, avoiding node repetition.\n",
    "\n",
    "    Args:\n",
    "    - path (str): A full path string with relations and line breaks.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (first_node, cleaned_path)\n",
    "    \"\"\"\n",
    "    if pd.isna(path) or not isinstance(path, str) or path.strip() == \"\":\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # Split by newline first to separate different transitions\n",
    "    lines = path.split(\"\\n\")\n",
    "\n",
    "    # Extract first node (from the first assertion before \"---\")\n",
    "    first_node = lines[0].split(\"---\")[0].strip()\n",
    "\n",
    "    # Process each transition and clean up relations\n",
    "    clean_nodes = []\n",
    "    for line in lines:\n",
    "        # Remove relations like ---xNext--->\n",
    "        parts = re.split(r\"---[a-zA-Z]+--->\", line)\n",
    "        if len(parts) == 2:  # Ensures it's a valid transition\n",
    "            node = parts[1].strip()  # Keep only the second part (destination)\n",
    "            clean_nodes.append(node)\n",
    "\n",
    "    # Remove duplicate nodes while preserving order\n",
    "    seen = set()\n",
    "    unique_nodes = [first_node]  # Start with the first node\n",
    "    for node in clean_nodes:\n",
    "        if node not in seen:\n",
    "            unique_nodes.append(node)\n",
    "            seen.add(node)\n",
    "\n",
    "    # Join nodes into a single path\n",
    "    reformatted_path = \" --> \".join(unique_nodes)\n",
    "    reformatted_path=format_path(reformatted_path)\n",
    "\n",
    "    return first_node, reformatted_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File paths\n",
    "input_file = \"/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/ckg_data/cleaned_arab_multilingual_extension_paths_final.xlsx\"\n",
    "output_file = \"/Cultural_Commonsense_Knowledge_Graph/ckg_result_arab/ckg_data/cleaned_arab_multilingual_extension_paths_final_reformated.xlsx\"\n",
    "\n",
    "# Read the Excel file with multiple sheets\n",
    "sheets_dict = pd.read_excel(input_file, sheet_name=None)\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets_dict.items():\n",
    "    if \"path_with_relations\" in df.columns:\n",
    "        # Apply the function and store results in new columns\n",
    "        df[['first_node', 'paths_without_relation']] = df['path_with_relations'].apply(\n",
    "            lambda x: pd.Series(extract_first_node_and_reformat(x)) if pd.notna(x) else pd.Series([\"\", \"\"])\n",
    "        )\n",
    "        sheets_dict[sheet_name] = df\n",
    "\n",
    "# Save the cleaned data back to Excel\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\":white_check_mark: The reformatted file has been saved at: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
